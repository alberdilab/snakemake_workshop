[["index.html", "Pipeline management with Snakemake Section for Hologenomics Workshop Chapter 1 Introduction 1.1 What is Snakemake? 1.2 Why Workflow Automation Matters 1.3 Problems Snakemake Solves 1.4 Relevant links", " Pipeline management with Snakemake Section for Hologenomics Workshop Antton Alberdi1 Last update: 2025-08-28 Chapter 1 Introduction Welcome to the Snakemake Workshop! In this session, we’ll explore how Snakemake can help you build scalable, reproducible, and automated workflows for data analysis. You’ll learn how to define tasks as rules, manage dependencies, and take full advantage of parallel execution on a high-performance cluster. By the end, you’ll be able to design workflows that are not only efficient, but also easy to share and reproduce. 1.1 What is Snakemake? Snakemake is a workflow management system that helps you automate and organize complex data processing tasks. Instead of manually running each step of an analysis in the correct order, you describe what needs to be done, the inputs it requires, and the outputs it should produce. Snakemake then determines the correct execution order, runs tasks in parallel where possible, and ensures that only the necessary steps are re-run when data or code changes. This approach makes your work more efficient, reproducible, and less prone to errors, whether you are working on a single computer or a high-performance computing cluster. In simple terms, it is like providing your computer with a detailed recipe: each step knows exactly which ingredients it needs, what it produces, and when it needs to be executed. 1.2 Why Workflow Automation Matters Reproducibility: In research and data-driven projects, results must be repeatable. Automation ensures that the same steps produce the same results every time, regardless of who runs them or where they are run. Efficiency: Automated workflows save time by eliminating repetitive manual commands and by re-running only the parts of an analysis that have changed. Error Reduction: Manual execution increases the risk of skipping steps, running them in the wrong order, or using outdated files. Automation enforces the correct sequence and data flow. Scalability: As projects grow, the number of steps and datasets can become unmanageable. Automated workflows make it practical to handle dozens or hundreds of tasks reliably. Documentation by Design: A well-defined workflow doubles as a detailed record of the entire analysis process, which is invaluable for collaboration, review, and publication. 1.3 Problems Snakemake Solves Managing Complex Dependencies: Snakemake automatically determines which steps depend on which others and runs them in the right order without manual intervention. Avoiding Unnecessary Work: It detects when inputs or code have changed and re-runs only the affected steps, saving both time and computing resources. Portability Across Systems: The same workflow can run unchanged on a laptop, a high-performance computing cluster, or even the cloud. Parallel Execution Without Extra Effort: Snakemake can execute independent tasks simultaneously, using all available cores or cluster nodes. Reproducibility and Transparency: By defining workflows in a readable text format, Snakemake makes analyses easy to share, inspect, and reproduce years later. 1.4 Relevant links Official Snakemake website: https://snakemake.github.io Snakemake documentation: https://snakemake.readthedocs.io Tutorial with examples: https://snakemake.readthedocs.io/en/stable/tutorial/tutorial.html Workflow examples: https://github.com/snakemake-workflows University of Copenhagen, ostaizka.aizpurua@sund.ku.dk↩︎ "],["getting-ready.html", "Chapter 2 Getting ready 2.1 Get Visual Studio Code ready 2.2 Create a SSH connection to Mjolnir 2.3 Open the remote explorer 2.4 Copy exercise files to your personal directory 2.5 Useful aliases 2.6 Screen session 2.7 Interactive job 2.8 Required modules 2.9 Let’s go!", " Chapter 2 Getting ready 2.1 Get Visual Studio Code ready Visual Studio Code (VS Code) is a lightweight, cross-platform code editor developed by Microsoft. It’s free, open source, and supports a wide range of programming languages. Download Visual studio code: https://code.visualstudio.com/download Using the extension manager, install the following plugins: Python by Microsoft Snakemake Language by Snakemake Remote - SSH by Microsoft Remote Explorer by Microsoft 2.2 Create a SSH connection to Mjolnir Ensure you are connected to the KU VPN Menu: View &gt; Command Palette &gt; Remote-SSH: Connect to Host Add New SSH Host ssh mjolnirgate.unicph.domain Type password This should have created a SSH connection to Mjolnir, which will enable you to create a Remote Explorewr. 2.3 Open the remote explorer Explorer Connect to remote &gt; Open Folder Use the following path: /maps/projects/course_2/people/{your_ku_id} 2.4 Copy exercise files to your personal directory cp -r /maps/projects/course_2/data/* . 2.5 Useful aliases Let’s declare some useful aliases to make the work a bit more pleasant. # Generate the file containing the aliases cat &lt;&lt;EOF &gt; aliases alias myjobs=&#39;squeue -u ${user} --format=&quot;%.18i %.9P %.30j %.8u %.8T %.10M %.9l %.6m %k&quot; --me&#39; alias screens=&#39;screen -list&#39; EOF # Source the aliases source aliases 2.6 Screen session Screen is a simple tool that keeps your command-line session running on a server even if your internet drops or you close your laptop. Think of it like putting your terminal in a locker—you can step away and later unlock it to find everything exactly where you left it. It also lets you make several “tabs” (windows) inside one SSH login and switch between them. On HPC clusters where tasks can run for hours, starting them inside Screen keeps them safe and easy to rejoin. It’s lightweight and saves you from losing work. To create a screen session for the entire duration of the workshop: screen -S snakemake_workshop To get out from the session, you must type ctrl+a d To re-enter the session, simply: screen -r snakemake_workshop And to close the session, just type exit. 2.7 Interactive job On a shared cluster like Mjolnir, the entry (login) nodes are the front door: they’re meant for quick, lightweight tasks like connecting, moving files, loading modules, editing scripts, and submitting jobs. They are shared by everyone and deliberately constrained (CPU, memory, long-running limits). Running heavy computations there slows the system for all users and can trigger automatic kills from cluster policies. An interactive session gives you a reserved slice of the compute nodes through the scheduler (e.g., Slurm). When you are in the entry node, the following command should return. hostname mjolnirgate01fl.unicph.domain In this workshop, each participant will get allocated 4 CPUs and 16GB of memory at node mjolnircomp14fl. When that time is over, the session will close automatically. srun \\ --job-name=snakemake_workshop \\ --account=teaching \\ --reservation=snakemake_hackaton \\ --partition=cpuqueue \\ --nodes=1 \\ --nodelist=mjolnircomp14fl \\ --time=08:00:00 \\ --mem=16G \\ --cpus-per-task=4 \\ --pty bash -I This will display for a few seconds, after which you will get connected to the computation node. srun: job XXXXXXXXX queued and waiting for resources To verify you are in the computing node the hostname command should now display: mjolnircomp14fl.unicph.domain To exit the interactive job, just type exit. 2.8 Required modules We will need to load two key modules to run the workshop. If you have your own conda, consider deactivating it temporarily to ensure the exercises run smoothly. # Ensure there is nothing loaded module purge # Load the relevant modules module load snakemake/9.4.0 module load miniconda/py39_23.1 # List the loaded modules module list Use should see exactly this: Currently Loaded Modulefiles: 1) miniconda/py39_23.1 2) snakemake/9.4.0 2.9 Let’s go! Now we are ready to go! "],["snakemake-basics.html", "Chapter 3 Snakemake basics 3.1 The Snakefile 3.2 The rules 3.3 Linking rules together 3.4 Executing the pipeline 3.5 Previsualising the pipeline 3.6 Exercise 1", " Chapter 3 Snakemake basics Snakemake is a rule-based workflow system. Each rule says: What it produces (the output) What it needs to produce that (the input) How to produce it (the command + parameters) Snakemake then figures out the right order to run everything and which parts can run at the same time. 3.1 The Snakefile The Snakefile is the heart of any Snakemake workflow. It is a plain text file—usually named Snakefile (with no file extension)—written in a mix of Snakemake’s workflow syntax and regular Python code. It serves as: A blueprint for your entire analysis pipeline A dependency map, describing what each step needs and produces An execution plan that Snakemake follows to run your tasks in the correct order The Snakefile is human-readable, version-controllable (e.g., with Git), and shareable. 3.2 The rules The Snakefile contains (at least) the rules that define the pipeline. 3.2.1 The ALL rule You typically start by specifying what you ultimately want to produce (plots, tables, reports) in the all rule: rule all: input: &quot;results/plot.png&quot; In this example, the pipeline will yield a plot.png file in the results directory. But you can also define multiple final outputs: rule all: input: &quot;results/plot.png&quot;, &quot;results/stats.tsv&quot; 3.2.2 A regular rule The regular rules define the path to reach the final outputs. Each rule at least must have a: Name: e.g., rule clean_data. Input: files or results needed. Output: files the step produces. Command: the command or script that performs the transformation. This rule, for example, outputs the row counts of a file to another file: rule count_lines: input: &quot;data/sample1.txt&quot; output: &quot;results/sample1.txt&quot; shell: &quot;wc -l {input} | awk &#39;{{print $1}}&#39; &gt; {output}&quot; 3.2.2.1 Name It must be unique, and ideally informative, as it will be used to identify the rules in the overview files, logs, etc. If you use repeated names, snakemake will complain and stop working. 3.2.2.2 Input and output Snakemake lets you describe input and output files in several convenient ways. 3.2.2.2.1 Single file rule count_lines: input: &quot;data/sample1.txt&quot; output: &quot;refs/genome.fa.fai&quot; shell: &quot;samtools faidx {input}&quot; or rule count_lines: input: &quot;data/sample1.txt&quot; output: &quot;refs/genome.fa.fai&quot; shell: &quot;samtools faidx {input}&quot; 3.2.2.2.2 Multiple unordered files rule count_lines: input: [&quot;results/a.counts.txt&quot;, &quot;results/b.counts.txt&quot;] output: &quot;results/merged.tsv&quot; shell: &quot;cat {input} &gt; {output}&quot; In this case, {input} expands to a space-separated list, this example resulting in: &quot;cat results/a.counts.txt results/b.counts.txt &gt; results/merged.tsv&quot; 3.2.2.2.3 Named children (recommended for clarity) rule count_lines: input: read1=&quot;reads/sample1_R1.fastq.gz&quot;, read2=&quot;reads/sample1_R2.fastq.gz&quot;, ref=&quot;refs/genome.fa&quot; output: bam=&quot;results/sample1.bam&quot; shell: &quot;bwa mem {input.ref} {input.read1} {input.read2} | &quot; &quot;samtools view -b -o {output.bam} 2&gt; {log.err}&quot; Use dot notation like {input.read1}, {output.bam}, etc. 3.2.2.3 Command Snakemake gives you several ways to express the command of a rule. 3.2.2.3.1 Shell The most common one is a regular shell command: rule count_lines: input: &quot;data/sample1.txt&quot; output: &quot;results/sample1.txt&quot; shell: &quot;wc -l {input} | awk &#39;{{print $1}}&#39; &gt; {output}&quot; The command definition can also be more complex, and expressed in multiple lines: rule count_lines: input: &quot;data/sample1.txt&quot; output: &quot;results/sample1.txt&quot; shell: &quot;&quot;&quot; coverm genome \\ -b {input} \\ -s _ \\ -m relative_abundance count \\ --min-covered-fraction 0 \\ &gt; {output} if [ $(stat -c &#39;%s&#39; {input}) -lt 1 ] then rm {input} fi &quot;&quot;&quot; 3.2.2.3.2 Script Or can run python scripts instead: rule count_lines: input: &quot;data/sample1.txt&quot; output: &quot;results/sample1.txt&quot; script: &quot;scripts/normalize.py&quot; 3.2.2.3.3 Run Or an inline Python block: rule concat: input: [&quot;pieces/a.txt&quot;, &quot;pieces/b.txt&quot;] output: &quot;all.txt&quot; run: with open(output[0], &quot;w&quot;) as out: for p in input: out.write(open(p).read()) 3.2.2.4 Child arguments 3.3 Linking rules together One of Snakemake’s most powerful features is that you don’t need to manually tell it the order in which to run rules. Instead, Snakemake infers the correct sequence automatically by looking at: The output files of one rule The input files of another When the output of one rule matches the input of another, Snakemake creates a connection between them—this connection is a dependency. Let’s see an example: rule all: input: &quot;results/summary.txt&quot; rule preprocess: input: &quot;raw/data.csv&quot; output: &quot;processed/data_clean.csv&quot; shell: &quot;python scripts/clean_data.py {input} {output}&quot; rule summarize: input: &quot;processed/data_clean.csv&quot; output: &quot;results/summary.txt&quot; shell: &quot;python scripts/summarize.py {input} {output}&quot; 3.4 Executing the pipeline The execution of the pipeline is very straightforward. From the command line, in the directory containing your Snakefile, just run: snakemake By default, Snakemake looks for a file named Snakefile in the current directory, or in the workflow directory, both with and without a capital S. snakefile Snakefile workflow/snakefile workflow/Snakefile It is also possible to define a different name for the snakefile, usually with the extension .smk. If so, you need to tell snakemake where the snakefile is: snakemake -s mydirectory/mysnakefile.smk 3.5 Previsualising the pipeline Before running a workflow, it is often helpful to preview what Snakemake will do. This allows you to verify that all dependencies are correctly linked, that no unexpected steps are included, and that your final targets are well defined. These tools are especially useful when developing or modifying workflows, ensuring that your Snakefile behaves as intended before committing computational resources. 3.5.0.1 Dry run Shows the planned execution without actually running any commands. This lists all jobs that would be executed and their order. snakemake --dry-run 3.5.0.2 DAG (Directed Acyclic Graph) visualization Generates a visual map of the workflow, illustrating how rules are connected. The resulting diagram helps you check for missing dependencies, unexpected branches, or redundant steps. snakemake --dag | dot -Tpng &gt; dag.png 3.6 Exercise 1 In the directory exercise1, you will find a data file and a snakefile with a few errors. Try to fix it to get the output file counts.tsv containing the following simple result: counts.tsv 7 "],["expanding-your-workflow.html", "Chapter 4 Expanding your workflow 4.1 Standard workflow structure 4.2 Configuration files 4.3 Modularisation of the Snakefile 4.4 Using custom environments 4.5 Exercise 2", " Chapter 4 Expanding your workflow Once you understand the basics of Snakemake—rules, inputs, outputs, and dependencies—you can start designing larger, more flexible workflows. This involves following a clear directory structure, using configuration files, adding wildcards for multiple datasets, and organizing rules for maintainability. 4.1 Standard workflow structure A well-organized Snakemake project usually follows a layout that keeps the workflow file, scripts, configurations, and results separate: workflow/ ├── Snakefile # Main workflow definition ├── config.yaml # Workflow configuration ├── rules/ # Additional rule files (optional) │ ├── preprocess.smk │ └── analysis.smk ├── scripts/ # Helper scripts (Python, R, Bash, etc.) │ ├── clean_data.py │ └── analyze.R ├── envs/ # Conda environment definitions │ ├── preprocess_env.yaml │ └── analysis_env.yaml ├── data/ # Input data │ └── reads/ │ └── reference/ └── results/ # Output files 4.2 Configuration files Instead of hardcoding parameters or file paths into the Snakefile or into the Snakemake launching script, you can store them in a config.yaml file. You can use all standard YAML types: strings, numbers, booleans, lists, and nested dictionaries. workflow/ ├── config.yaml Here are some examples of information you can stored in a config file: config.yml # Execution parameters jobs: 100 latency-wait: 60 use-conda: true rerun-incomplete: true keep-going: true retries: 3 You can also chunk the configuration file by declaring child files: config.yml features: config/features.yml params: config/params.yml features.yml hosts: human: resources/reference/human_22_sub.fa.gz chicken: resources/reference/chicken_39_sub.fa.gz mag_catalogues: mag1: resources/reference/mags_sub.fa.gz params.yml preprocess: fastp: extra: &quot; --length_required 75 --trim_poly_g --trim_poly_x&quot; bowtie2: bowtie2_extra: &quot;&quot; samtools_extra: &quot;-m 1G&quot; quantify: coverm: genome: methods: [&quot;count&quot;, &quot;covered_bases&quot;] separator: &quot;@&quot; extra: &quot;--min-covered-fraction 0&quot; 4.2.1 Using config information You can then access the information stored in the config file(s). 4.3 Modularisation of the Snakefile As workflows grow, a single Snakefile can become long and difficult to navigate. To improve maintainability, readability, and collaboration, you can split the workflow into multiple rule files and include them in your main Snakefile. These are typically stored in the workflow/rules directory: workflow/ ├── rules/ │ ├── preprocess.smk │ └── analysis.smk Instead of defining every rule in the Snakefile, rules are fetched from related files: Snakefile configfile: &quot;config.yaml&quot; include: &quot;rules/preprocess.smk&quot; include: &quot;rules/align.smk&quot; rule all: input: &quot;results/final_report.html&quot; rules/preprocess.smk rule preprocess: input: &quot;data/raw/{sample}.csv&quot; output: &quot;data/clean/{sample}.csv&quot; shell: &quot;python scripts/clean_data.py {input} {output}&quot; rules/align.smk rule align: input: reads=&quot;data/clean/{sample}.csv&quot;, ref=config[&quot;reference&quot;] output: &quot;results/{sample}.bam&quot; shell: &quot;bwa mem {input.ref} {input.reads} | samtools view -b -o {output}&quot; 4.4 Using custom environments Robust workflows separate software environments from pipeline logic. Snakemake makes this easy with per-rule environments so each step has exactly the tools and versions it needs—improving reproducibility and avoiding “it works on my machine”. 4.4.1 HPC environment modules Mjolnir, as many other HPCs, provide modules that can be loaded per rule: rule align: input: ref=config[&quot;references&quot;][&quot;genome_fa&quot;], reads=&quot;data/{sample}.fastq.gz&quot; output: bam=&quot;results/{sample}.bam&quot; envmodules: &quot;fastqc/0.12.1&quot;, &quot;java/17&quot; shell: &quot;fastqc -o results {input}&quot; To use environment modules, it’s necessary to use the --use-conda argument when launching snakemake: snakemake --use-envmodules 4.4.2 Conda environments Conda environments are typically declared and stored in the workflow/envs directory. workflow/ ├── envs/ # Conda environment definitions │ ├── preprocess.yaml │ └── analysis.yaml Environments are declared as small YAML files: workflow/envs/preprocess.yaml channels: - conda-forge - bioconda - defaults dependencies: - bwa=0.7.17 - samtools=1.20 - python=3.11 - pip - pip: - pysam==0.22.0 And then requested in the respective rule in the snakefile: rule align: input: ref=config[&quot;references&quot;][&quot;genome_fa&quot;], reads=&quot;data/{sample}.fastq.gz&quot; output: bam=&quot;results/{sample}.bam&quot; conda: &quot;envs/preprocess.yaml&quot; shell: &quot;bwa mem -t {threads} {input.ref} {input.reads} | samtools view -b -o {output.bam}&quot; To use conda environments, it’s necessary to use the --use-conda argument when launching snakemake: snakemake --use-conda 4.4.3 Other compartimentalisation options It is also possible to use Singularity containers and Docker images following a similar logic. 4.5 Exercise 2 In the directory exercise2, you will find a more complex snakemake worflow structure with a few errors. Note this time the workflow requires the use of conda environments, so make sure you launch snakemake using: snakemake --use-conda Try to fix it to get the output file results/counts.tsv containing the following simple result: results/counts.tsv 16 "],["advanced-rule-parameters.html", "Chapter 5 Advanced rule parameters 5.1 Params 5.2 Threads 5.3 Advanced inputs 5.4 Exercise 3", " Chapter 5 Advanced rule parameters 5.1 Params params are a convenient way to pass options or constants to your command. They are not files (unlike input/output) and not automatically tracked by Snakemake (so changes don’t normally trigger re-runs). rule align_reads: input: reads=&quot;data/sample.fastq.gz&quot;, ref=&quot;ref/genome.fasta&quot; output: &quot;results/sample.bam&quot; params: threads=4, extra=&quot;--very-sensitive&quot; shell: &quot;bowtie2 -x {input.ref} -U {input.reads} {params.extra} &quot; &quot;-p {params.threads} | samtools view -bS - &gt; {output}&quot; params.threads and params.extra are just strings/numbers you defined. They expand like {params.threads} inside the shell:. 5.2 Threads threads declares how many CPU cores a rule can use. Snakemake uses this information to schedule jobs across available cores when you run with &gt;1 -c/--cores. Unlike params, threads is resource-aware: Snakemake respects it when deciding how many jobs to run in parallel. rule sort_bam: input: &quot;results/sample.bam&quot; output: &quot;results/sample.sorted.bam&quot; threads: 4 shell: &quot;samtools sort -@ {threads} -o {output} {input}&quot; threads: 4 tells Snakemake this job needs 4 cores. Inside shell:, you expand {threads} (not {params.threads}!). If you launch Snakemake with snakemake --cores 8, Snakemake can only run two of these jobs at once (because each consumes 4 cores). 5.3 Advanced inputs 5.3.1 Expansions Use expand() to build input lists for the all rule or aggregations. SAMPLES = [&quot;S1&quot;, &quot;S2&quot;, &quot;S3&quot;] rule all: input: expand(&quot;qc/{sample}_fastqc.html&quot;, sample=SAMPLES) The above code results in: rule all: input: &quot;qc/S1_fastqc.html&quot;, &quot;qc/S2_fastqc.html&quot;, &quot;qc/S3_fastqc.html&quot; 5.3.2 Pattern-building helpers Use glob_wildcards to discover samples containing a certain pattern: from snakemake.io import glob_wildcards SAMPLES = glob_wildcards(&quot;reads/{sample}.fastq.gz&quot;).sample rule all: input: expand(&quot;qc/{sample}_fastqc.html&quot;, sample=SAMPLES) Get a list of file names in the directory reads that end with _1.fq.gz: from pathlib import Path SAMPLES = sorted({f.stem.rsplit(&quot;_&quot;, 1)[0] for f in Path(&quot;reads&quot;).glob(&quot;*_1.fq.gz&quot;)}) rule all: input: expand(&quot;results/{sample}.tsv&quot;, sample=SAMPLES) 5.3.3 Wildcards rule align: input: lambda wc: [f&quot;reads/{wc.sample}_R1.fastq.gz&quot;, f&quot;reads/{wc.sample}_R2.fastq.gz&quot;] output: &quot;results/{sample}.bam&quot; 5.4 Exercise 3 In the directory exercise3, you will find a snakemake worflow that requires you to use wildcards. Note this time the workflow requires the use of conda environments, so make sure you launch snakemake using: snakemake --use-conda Try to fix it to get the output file results/multiqc/report.html. "],["checkpoints.html", "Chapter 6 Checkpoints 6.1 Exercise 4", " Chapter 6 Checkpoints Normally, Snakemake needs to know all inputs in advance so it can plan the DAG. But sometimes you don’t know the inputs until after another rule has run — e.g.: You split a file into an unknown number of chunks. You download a dataset whose contents aren’t known upfront. You parse a metadata file that wasn’t available when you wrote the workflow. This is where checkpoints come in. A checkpoint is like a rule, but Snakemake pauses the DAG here. After the checkpoint runs, you can dynamically discover its outputs. Then you can use a dynamic input function to create follow-up jobs. Checkpoints are defined by using checkpoint rather than rule. checkpoint split_file: input: &quot;data/largefile.txt&quot; output: directory(&quot;chunks&quot;) shell: &quot;&quot;&quot; mkdir -p {output} split -l 1000 {input} {output}/part_ &quot;&quot;&quot; As a result, we should expect an unknown number of files within the folder ‘chunks’. To process each of them one by one, we need to use wildcards. rule process_chunk: input: &quot;chunks/{chunk}&quot; output: &quot;processed/{chunk}.out&quot; shell: &quot;wc -l {input} &gt; {output}&quot; However, snakemake is still unable to guess what the wildcards, until we provide the required information to create the updated DAG. The way to do this, is to define a functions that gathers the newly created files’ names. def chunk_ids(): ck = checkpoints.split_file.get() #Ensure split_file has run, and gets the output directory ids = glob_wildcards(join(ck.output[0], &quot;{chunk}&quot;)).chunk # Gets file id&#39;s from the output directory return ids This function, which is executed in the following rule, creates a python list like ‘[“part_aa”, “part_ab”]’, which is used to generate the updated DAG. rule merge_results: input: lambda wc: expand(&quot;processed/{chunk}.out&quot;, chunk=chunk_ids()) output: &quot;results/merged.txt&quot; shell: &quot;cat {input} &gt; {output}&quot; rule all: input: &quot;results/merged.txt&quot; 6.1 Exercise 4 "],["slurm-integration.html", "Chapter 7 SLURM integration 7.1 Examples 7.2 Attempts 7.3 Dynamic resources 7.4 Exercise 5", " Chapter 7 SLURM integration SLURM (Simple Linux Utility for Resource Management) is a widely used open-source job scheduler and workload manager for high-performance computing (HPC) clusters. It handles three main tasks: Resource allocation deciding which jobs run on which nodes. Job scheduling prioritising and queuing jobs. Job execution launching and monitoring jobs across compute nodes. Snakemake can submit workflow rules directly to SLURM. When a rule is executed this way, Snakemake doesn’t run the job itself; instead, it generates an sbatch command with the appropriate resource requests and submits it to SLURM. From Snakemake 8 onwards, it requires installation of the (executor plugin)[https://anaconda.org/bioconda/snakemake-executor-plugin-slurm]. 7.1 Examples Standalone example: - https://github.com/alberdilab/genotyping Integrated in installable software: - https://github.com/alberdilab/barcodemapper ## Defining resources resources declare arbitrary, countable things a rule needs to run (memory, GPUs, disk, time, etc.). When executing Snakemake with SLURM it is essential to define the resources that will be used for each job, so that SLURM can properly allocated resources and prioritise jobs in the queue. rule variant_calling: input: &quot;results/sample.sorted.bam&quot; output: &quot;results/sample.vcf&quot; threads: 8 resources: mem_mb=16000, runtime=10 shell: &quot;freebayes -f ref/genome.fasta {input} &gt; {output}&quot; resources: mem_mb=16000 indicates that the job needs 16 GB RAM. resources: runtime=10 indicates that the job must be done within 10 minutes. 7.2 Attempts Some jobs fail occasionally for reasons outside your workflow. This is usually because the job required more memory than allocated, or more time than specified to accomplish the task. In the case of downloads from the internet, it could also be because the connection failed. attempts lets you tell Snakemake: “If this rule fails, try again up to N times before stopping”. For example, if a rule is declared to download a file from the internet and the connection fails, snakemake will try up to three times to accomplish the task. The number of attempts can be declared for each rule: rule variant_calling: input: &quot;results/sample.sorted.bam&quot; output: &quot;results/sample.vcf&quot; resources: mem_mb=16000, runtime=10 attempts: 3 shell: &quot;freebayes -f ref/genome.fasta {input} &gt; {output}&quot; Or a global number of attempts can be defined using the flag –retries. snakemake --retries 3 7.3 Dynamic resources When using Snakemake on an HPC with a queuing system, job failures often come from insufficient resources. To better address this issue, Snakemake can adjust resources dynamically on each try by specifying rules for scaling. rule variant_calling: input: &quot;results/sample.sorted.bam&quot; output: &quot;results/sample.vcf&quot; threads: 8 attempts: 3 resources: mem_mb=lambda wildcards, attempt: 4000 * attempt, runtime=lambda wildcard, sattempt: 10 * attempt shell: &quot;freebayes -f ref/genome.fasta {input} &gt; {output}&quot; In the above example: First attempt: 4000 Mb of memory and 10 minutes Second attempt: 8000 Mb of memory and 20 minutes Third attempt: 12000 Mb of memory and 30 minutes The scaling can also be exponential using the following function: rule variant_calling: input: &quot;results/sample.sorted.bam&quot; output: &quot;results/sample.vcf&quot; threads: 8 attempts: 3 resources: mem_mb=lambda wildcards, attempt: 4000 * 2 ** (attempt - 1), runtime=lambda wildcard, attempt: 10 * 2 ** (attempt - 1) shell: &quot;freebayes -f ref/genome.fasta {input} &gt; {output}&quot; In the new example, the resources will double in every attempt: First attempt: 4000*2^0 = 4000 Mb of memory and 10 minutes Second attempt: 4000*2^1 = 8000 Mb of memory and 20 minutes Third attempt: 4000*2^2 = 16000 Mb of memory and 40 minutes However, the required resources can vary significantly within project: some input files might be 10 Mb, while others 100 Mb, requiring many more resources. The input.size_mb options allows for calculating the resources based on the size of the input files: rule variant_calling: input: &quot;results/sample.sorted.bam&quot; output: &quot;results/sample.vcf&quot; threads: 8 attempts: 3 resources: mem_mb=lambda wildcards, input, attempt: max(64*1024, int(input.size_mb * 10) * 2 ** (attempt - 1)), runtime=lambda wildcards, input, attempt: max(15, int(input.size_mb / 50) * 2 ** (attempt - 1)) shell: &quot;freebayes -f ref/genome.fasta {input} &gt; {output}&quot; When defining such dynamic resources, it’s not uncommon to require resources that are two small or so big that cannot be satisfied. The functions max() and min() enable setting minimum and maximum values within the resources can fluctuate. rule variant_calling: input: &quot;results/sample.sorted.bam&quot; output: &quot;results/sample.vcf&quot; threads: 8 attempts: 3 resources: mem_mb=lambda wildcards, input, attempt: max(64*1024, int(input.size_mb * 10) * 2 ** (attempt - 1)), runtime=lambda wildcards, input, attempt: min(1440, int(input.size_mb / 50) * 2 ** (attempt - 1)) shell: &quot;freebayes -f ref/genome.fasta {input} &gt; {output}&quot; In the above example, the minimum amount of memory is set to 64 Gb. If the dynamic calculation is below that number, 64 Gb will be used instead. The maximum amount of time is set to 1440 minutes (a day). If the dynamic calculation exceds that number, the minimum value will be used, that is 1440. ## Executing Snakemake with SLURM 7.3.1 Direct specification snakemake --executor slurm 7.3.2 Profile specification This option gives you much more control over how to execute Snakemake jobs via SLURM. 7.3.2.1 Create profile config Prepare a profile config file specifying slurm execution profile/slurm/config.yaml executor: slurm jobs: 50 latency-wait: 60 use-conda: true rerun-incomplete: true rerun-trigger: mtime keep-going: false retries: 3 default-resources: mem_mb: 8 * 1024 runtime: 10 * 2 ** (attempt - 1) max-threads: 8 In addition to the default resources, it is also possible to define rule-specific resources in the profile. executor: slurm jobs: 50 latency-wait: 60 use-conda: true rerun-incomplete: true rerun-trigger: mtime keep-going: false retries: 3 default-resources: mem_mb: 8 * 1024 runtime: 10 * 2 ** (attempt - 1) max-threads: 8 set-threads: variant_calling: 8 set-resources: variant_calling: mem_mb: 32 * 1024 * 2 ** (attempt - 1) runtime: 12 * 2 ** (attempt - 1) 7.3.2.2 Execute Snakemake with profile snakemake --workflow-profile profile/slurm 7.4 Exercise 5 Generate the following output files by fixing the mistakes in the snakefile: distance_matrix.txt pcoa.png heatmap.png depth.tsv "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
